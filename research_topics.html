<ul class="research-topics-list">
    <li>
      <p>
        <strong>Generalization of modern neural networks:</strong>
        [<a href="#paper3" style="color: #1e90ff;">3</a>, <a href="#paper5" style="color: #1e90ff;">5</a>]
        <a href="#" onclick="toggleDetails('details3'); return false;" style="color: #20c997;">Click to unroll a quick description</a>
      </p>
      <p id="details3" style="display: none;">
        The so-called path-lifting and its norm, the path-norm, are promising theoretical tools that have emerged to analyze ReLU neural networks by taking into account their symmetries. These tools have already been used to derive generalization bounds, identifiability guarantees, and characterizations of properties of the training dynamics. In [<a href="#paper3" style="color: #1e90ff;">3</a>], we made these tools fully compatible with modern networks such as ResNets, VGGs, U-nets, ReLU MobileNets, Alexnet, and many more. We further derived a new generalization bound for these models based on the path-norm, that recovers or improves on all the previous bounds of this type. Since this is the first bound of this type that is valid for modern networks, we could assess it on ResNets trained on ImageNet and identify the gap with practice, as well as promising leads to bridge this gap  [<a href="#paper3" style="color: #1e90ff;">3</a>]. The paper [<a href="#paper5" style="color: #1e90ff;">5</a>] gives a complementary perspective on the use of the path-lifting related tools for generalization. Just to give you a teaser of how powerful these tools are, an easy consequence of the main result (Theorem 3.1) of [<a href="#paper5" style="color: #1e90ff;">5</a>] is that if you have two weight vectors of your network that have the same signs and the same path-lifting vectors, then they must correspond to the same input-output mapping. This results tells you that the path-lifting characterizes the network function up to the sign of the weights, while having more desirable properties that the weights themselves (e.g. being inherently invariant to rescaling symmetries).
      </p>
    </li>

    <li>
      <p>
        <strong>Pruning invariant to rescaling symmetries:</strong>
        [<a href="#paper5" style="color: #1e90ff;">5</a>]
        <a href="#" onclick="toggleDetails('details4'); return false;" style="color: #20c997;">Click to unroll a quick description</a>
      </p>
      <p id="details4" style="display: none;">
        In [<a href="#paper5" style="color: #1e90ff;">5</a>], we further extended the path-lifting toolkit that we introduced in [<a href="#paper3" style="color: #1e90ff;">3</a>] for modern networks. The first contribution of [<a href="#paper5" style="color: #1e90ff;">5</a>] is to introduce a natural (rescaling-invariant) metric based on the path-lifting, and to show that it yields an upper bound on the distance between two realizations of a network. The second contribution of [<a href="#paper4" style="color: #1e90ff;">5</a>] is to shed the light on theoretical and practical consequences of this new property. In particular, we provide a new pruning algorithm that is invariant under symmetries and that matches the accuracy of the standard magnitude pruning method when applied to ResNets trained on Imagenet in the lottery ticket context. The emerging theory of the path-lifting provides a promising foundation for future theoretical analysis of this pruning method.
      </p>
    </li>

    <li>
      <p>
        <strong>Fast inference on GPU using structured sparsity:</strong>
        [<a href="#paper4" style="color: #1e90ff;">4</a>]
        <a href="#" onclick="toggleDetails('details5'); return false;" style="color: #20c997;">Click to unroll a quick description</a>
      </p>
      <p id="details5" style="display: none;">
        In this project, we accelerate the inference of neural networks using sparse butterfly matrices.<br/><br/>
        <em>Why butterfly matrices?</em><br/>
        1) Neural networks with butterfly matrices have been empirically found to match the accuracy of standard networks in various settings.<br/>
        2) Butterfly matrices are very expressive and quite ubiquitous in signal processing, for instance they appear in the Discrete Fourier and Hadamard transforms.<br/>
        3) Butterfly matrices have a sparsity that is structured (the support is defined with Kronecker products, the number of nonzero coefficients is the same in every row/column etc.) which helps a lot to achieve efficient implementations.<br/>
        4) Butterfly matrices have a sub-quadratic (even quasi-linear) theoretical matrix-vector multiplication complexity.<br/><br/>
        Matrix multiplication is at the heart of the inference and training of neural networks. Because of that, butterfly matrices show promise to reduce the resource demands of neural networks. However, their current practical benefit (timewise and energywise) remains unclear due to a lack of reported numerical results. In [<a href="#paper5" style="color: #1e90ff;">4</a>], we extensively benchmark existing baseline implementations and find out that they spend up to 50% of their time on memory operations. We show that these memory operations can be optimized by introducing a new CUDA kernel that minimizes the transfers between the different levels of GPU memory, achieving a median speed-up factor of 1.4 while also reducing energy consumption (median of 0.85). We also concretely demonstrate how the new kernel can speed up the inference of neural networks.
      </p>

    </li>

      <li>
        <p>
          <strong>Privacy of sparse neural networks:</strong>
          [<a href="#paper2" style="color: #1e90ff;">2</a>]
          <a href="#" onclick="toggleDetails('details2'); return false;" style="color: #20c997;">Click to unroll a quick description</a>
        </p>
        <p id="details2" style="display: none;">
          There is currently no consensus on whether or not training sparse neural networks improves the privacy of the training dataset. Our empirical results in [<a href="#paper2" style="color: #1e90ff;">2</a>] show that there is not only a correlation between the network sparsity and privacy, but also with the network accuracy. We conclude that experiments investigating this conjecture must carefully consider the correlation with accuracy, a factor that has unfortunately been overlooked in previous studies that we revisited.
        </p>
      </li>
      <li>
            <p>
              <strong>Approximation properties of ReLU neural networks:</strong>
              [<a href="#paper1" style="color: #1e90ff;">1</a>]
              <a href="#" onclick="toggleDetails('details1'); return false;" style="color: #20c997;">Click to unroll a quick description</a>
            </p>
            <p id="details1" style="display: none;">
              While the universal approximation theorem states that neural networks with arbitrary real weights can theoretically approximate any sufficiently smooth function, practical networks operate with specific floating-point constrained weights. In [<a href="#paper1" style="color: #1e90ff;">1</a>], we identify concrete bit-precision conditions under which floating-point constrained ReLU networks maintain the same approximation polynomial speeds as those with real weights.
            </p>
          </li>
  </ul>
